{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "approximate-pasta",
   "metadata": {},
   "source": [
    "# Nexthink AI Software Engineer assignment\n",
    "\n",
    "\n",
    "First, we thank you for the time and effort you will invest in this assignment. We believe that this exercise will help us understand your technical skills and expertise in the field of artifical intelligence and machine learning.\n",
    "\n",
    "You have seven days to complete the assignment. We estimate that the work required should take only a few hours at most. A jupyter notebook is provided as a starting point, but you are free to use any other format.\n",
    "\n",
    "Please pay attention to the code and the description of your approach and results. The code should be well-documented and easy to understand. Additionally, we expect that you will provide a clear description of your approach to the problem and the rationale behind your methodology. In case of doubt, you can take any assumption you think is reasonable but make sure you explain it well. Lastly, we encourage you to present your results in an easy-to-understand manner, including visualizations and other appropriate metrics. \n",
    "\n",
    "We wish you the best of luck in completing the exercises, and we look forward to reviewing your submission. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-efficiency",
   "metadata": {},
   "source": [
    "## 1. Uncovering topics behind news articles\n",
    "\n",
    "*You are a tech-savvy journalist tasked with classifying news articles into categories. To save time, you decide to use your machine learning skills to automate this process.*\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We provide a dataset (`news.jsonl`) containing around 210k news headlines between 2012 and 2022 from HuffPost. It contains the following attributes:\n",
    "\n",
    "- `link`: link to the original news article.\n",
    "- `headline`: the headline of the news article.\n",
    "- `category`: category in which the article was published.\n",
    "- `short_description`: Abstract of the news article.\n",
    "- `authors`: list of authors who contributed to the article.\n",
    "- `date`: publication date of the article.\n",
    "\n",
    "The objective is to automatically determine the categories behind news articles. You will solely make use of the `headline` attribute **OR** the `short_description` attribute.\n",
    "\n",
    "> For both parts below we expect you to explain and show that your solution works as expected (e.g., through metrics on a test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "developed-cocktail",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffpost.com/entry/covid-boosters-...</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "      <td>Carla K. Johnson, AP</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffpost.com/entry/american-airlin...</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "      <td>Mary Papenfuss</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-tweets...</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "      <td>Elyse Wanshel</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-parent...</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "      <td>Caroline Bologna</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffpost.com/entry/amy-cooper-lose...</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "      <td>Nina Golgowski</td>\n",
       "      <td>2022-09-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  https://www.huffpost.com/entry/covid-boosters-...   \n",
       "1  https://www.huffpost.com/entry/american-airlin...   \n",
       "2  https://www.huffpost.com/entry/funniest-tweets...   \n",
       "3  https://www.huffpost.com/entry/funniest-parent...   \n",
       "4  https://www.huffpost.com/entry/amy-cooper-lose...   \n",
       "\n",
       "                                            headline   category  \\\n",
       "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
       "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
       "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
       "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
       "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
       "\n",
       "                                   short_description               authors  \\\n",
       "0  Health experts said it is too early to predict...  Carla K. Johnson, AP   \n",
       "1  He was subdued by passengers and crew when he ...        Mary Papenfuss   \n",
       "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel   \n",
       "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna   \n",
       "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski   \n",
       "\n",
       "        date  \n",
       "0 2022-09-23  \n",
       "1 2022-09-23  \n",
       "2 2022-09-23  \n",
       "3 2022-09-23  \n",
       "4 2022-09-22  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "news_df = pd.read_json(\"data/news.jsonl\", lines=True)\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-principal",
   "metadata": {},
   "source": [
    "### Known categories\n",
    "\n",
    "You will first assume that you know the categories (e.g., the unique values of the `category` attribute). Train a model able to correctly classify the headline or description of news articles into the correct category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a48986-66fc-4ea6-adab-f39109e31cf6",
   "metadata": {},
   "source": [
    "#### SOLUTION: \n",
    "#### For this solution XGBOOST model is used after reducing the number of categories by normalizing them. \n",
    "#### The total number of remaining categories are 25.\n",
    "#### With tfidf features extraction using with xgboost we get 0.48 as the weighted average F1 score. \n",
    "#### It is a good initial score considering the variance and the number of categories.\n",
    "#### Transformer based techniques can have better performance in cases like these.\n",
    "#### XGBOOST was chosen as it provides inherent class weights adjustment and is also comparatively robust to imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "seven-shape",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/om/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/om/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/om/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import words, wordnet\n",
    "import xgboost as xgb\n",
    "import faiss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# Downloading necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b30bc041-0e56-4519-9f1b-03cff1de3e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"A simple preprocessing utility\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Removing Stop Words and Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stopwords.words('english')]\n",
    "    \n",
    "    # Removing punctuation\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # Convert tokens back to string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aa9ae94-4e91-4588-b7f1-b4c3d0650c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess news_df short description\n",
    "news_df[\"short_desc_preprocessed\"] = news_df[\"short_description\"].apply(lambda x:preprocess_text(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6deb8d70-85ee-4923-8ffc-f2a301c89478",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = news_df[news_df['short_description'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7601cceb-a6f3-41b7-8c20-d44265793ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the category normalization to combine categories of same topics\n",
    "\n",
    "# 1. Combine \"GREEN\" and \"ENVIRONMENT\" into \"ENVIRONMENT\"\n",
    "news_df['category'] = news_df['category'].replace(['GREEN', 'ENVIRONMENT'], 'ENVIRONMENT')\n",
    "\n",
    "# 2. Combine \"QUEER VOICES\", \"BLACK VOICES\", and \"LATINO VOICES\" into \"DIVERSE VOICES\"\n",
    "news_df['category'] = news_df['category'].replace(['QUEER VOICES', 'BLACK VOICES', 'LATINO VOICES'], 'DIVERSE VOICES')\n",
    "\n",
    "# 3. Combine \"WORLDPOST\" and \"THE WORLDPOST\" and \"U.S NEWS\" into \"WORLD NEWS\". \n",
    "# note: Just an experiment considering that US News has US events which are sometime similar to world events\n",
    "news_df['category'] = news_df['category'].replace(['WORLDPOST', 'THE WORLDPOST','U.S. NEWS'], 'WORLD NEWS')\n",
    "\n",
    "# 4. Combine \"PARENTING\" and \"PARENTS\" into \"PARENTING\"\n",
    "news_df['category'] = news_df['category'].replace(['PARENTING', 'PARENTS'], 'PARENTING')\n",
    "\n",
    "# 5. Combine \"ARTS\", \"ARTS & CULTURE\", and \"CULTURE & ARTS\" into \"ARTS & CULTURE\"\n",
    "news_df['category'] = news_df['category'].replace(['ARTS', 'ARTS & CULTURE', 'CULTURE & ARTS'], 'ARTS & CULTURE')\n",
    "\n",
    "# 6. Combine \"TECH\" and \"SCIENCE\" into \"TECH & SCIENCE\"\n",
    "news_df['category'] = news_df['category'].replace(['TECH', 'SCIENCE'], 'TECH & SCIENCE')\n",
    "\n",
    "# 7. Combine \"TASTE\" and \"FOOD & DRINK\" into \"FOOD & DRINK\"\n",
    "news_df['category'] = news_df['category'].replace(['TASTE', 'FOOD & DRINK'], 'FOOD & DRINK')\n",
    "\n",
    "# 8. Combine \"MONEY\", \"BUSINESS\" into \"BUSINESS & FINANCE\"\n",
    "news_df['category'] = news_df['category'].replace(['MONEY', 'BUSINESS'], 'BUSINESS & FINANCE')\n",
    "\n",
    "# 9. Combine \"COLLEGE\", \"EDUCATION\" into \"EDUCATION\"\n",
    "news_df['category'] = news_df['category'].replace(['COLLEGE', 'EDUCATION'], 'EDUCATION')\n",
    "\n",
    "# 10. Combine \"STYLE\", \"STYLE AND BEAUTY\" into \"STYLE & BEAUTY\"\n",
    "news_df['category'] = news_df['category'].replace(['STYLE', 'STYLE & BEAUTY', 'STYLE AND BEAUTY'], 'STYLE & BEAUTY')\n",
    "\n",
    "# 11. Combine \"GREEN\" and \"ENVIRONMENT\" into \"ENVIRONMENT\"\n",
    "news_df['category'] = news_df['category'].replace(['WELLNESS', 'HEALTHY LIVING'], 'WELLNESS')\n",
    "\n",
    "# 12. Combine \"WEIRD NEWS\", \"GOOD NEWS\", \"FIFTY\" into \"MISCELLANEOUS\"\n",
    "news_df['category'] = news_df['category'].replace(['WEIRD NEWS', 'GOOD NEWS', 'FIFTY'], 'MISCELLANEOUS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80b9d34a-7181-4766-af00-6d15a959ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(news_df['category'].unique())\n",
    "X = news_df['short_desc_preprocessed']\n",
    "y = label_encoder.transform(news_df['category'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "964ccce3-36f9-4245-8f40-b425de99ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Preprocessing and Tokenization\n",
    "tfidf = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "X_tfidf_train = tfidf.fit_transform(X_train)\n",
    "X_tfidf_test = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "499c9439-7f86-43a2-bf11-55242556949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compute class weights based on median\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train) # \"Balanced\" is initialisation The next step changes the weights\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "weights = [class_weights_dict[label] for label in y_train]\n",
    "\n",
    "# Adjust weights based on median of all weights\n",
    "weights_median = np.median(class_weights)\n",
    "weights = [weight / weights_median for weight in weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f082484-06be-4f5d-8593-01a8cdfcfec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Train the XGBoost classifier with class weights\n",
    "dtrain = xgb.DMatrix(X_tfidf_train, label=y_train, weight=weights)\n",
    "dtest = xgb.DMatrix(X_tfidf_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': len(news_df['category'].unique()),\n",
    "    'max_depth': 10,\n",
    "    'silent': 1,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'subsample': 0.7,\n",
    "    'learning_rate': 0.05,\n",
    "    'verbosity' : 0\n",
    "}\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "num_round = 1000\n",
    "\n",
    "xgb_model = xgb.train(params, dtrain, num_round, watchlist, early_stopping_rounds=10,verbose_eval=0)\n",
    "\n",
    "# 4. Evaluate the model\n",
    "y_pred = xgb_model.predict(dtest)\n",
    "report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0fc0ff74-0d2a-4549-8312-a844a675da89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "    ARTS & CULTURE       0.28      0.42      0.34       653\n",
      "BUSINESS & FINANCE       0.40      0.47      0.43      1377\n",
      "            COMEDY       0.15      0.24      0.19       929\n",
      "             CRIME       0.25      0.48      0.33       566\n",
      "    DIVERSE VOICES       0.49      0.28      0.36      2160\n",
      "           DIVORCE       0.52      0.59      0.55       685\n",
      "         EDUCATION       0.25      0.49      0.33       365\n",
      "     ENTERTAINMENT       0.40      0.38      0.39      2955\n",
      "       ENVIRONMENT       0.33      0.43      0.37       698\n",
      "      FOOD & DRINK       0.57      0.62      0.59      1654\n",
      "     HOME & LIVING       0.33      0.48      0.39       863\n",
      "            IMPACT       0.20      0.25      0.22       617\n",
      "             MEDIA       0.19      0.37      0.25       481\n",
      "     MISCELLANEOUS       0.13      0.18      0.15       879\n",
      "         PARENTING       0.57      0.52      0.55      2469\n",
      "          POLITICS       0.79      0.47      0.59      6488\n",
      "          RELIGION       0.26      0.44      0.33       376\n",
      "            SPORTS       0.32      0.51      0.39       883\n",
      "    STYLE & BEAUTY       0.68      0.59      0.63      2274\n",
      "    TECH & SCIENCE       0.27      0.39      0.32       781\n",
      "            TRAVEL       0.57      0.57      0.57      1884\n",
      "          WEDDINGS       0.57      0.62      0.60       731\n",
      "          WELLNESS       0.72      0.54      0.62      4642\n",
      "             WOMEN       0.14      0.24      0.18       637\n",
      "        WORLD NEWS       0.48      0.44      0.46      1916\n",
      "\n",
      "          accuracy                           0.46     37963\n",
      "         macro avg       0.39      0.44      0.40     37963\n",
      "      weighted avg       0.53      0.46      0.48     37963\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c345c908-ae63-44d5-938c-ba7d509768a9",
   "metadata": {},
   "source": [
    "#### BERT based solution found here: https://colab.research.google.com/drive/1lQnviwohO5Rs71pM_LNpEnzuAtdVuMkl#scrollTo=wENRYGOJuq7S \n",
    "#### It beats the XGBOOST by 23 percentage points. Please have a look at the link."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb3878f-9996-4b10-8246-4ee6117e028a",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-buddy",
   "metadata": {},
   "source": [
    "### Unkown categories\n",
    "\n",
    "You will next assume that the categories are unknown (e.g., you are NOT allowed to use the `category` attribute). However, you CAN assume that the number of categories is known.\n",
    "\n",
    "Your solution should:\n",
    "- Identify news headlines that belong to the same category\n",
    "- Provide a human-understandable representation of each category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc2b166-a92b-4f84-b6fd-b91880ecd59c",
   "metadata": {},
   "source": [
    "#### Solution: \n",
    "#### For this solution I use a FAISS based clustering with TFIDF features with 1000 max features due to memory contraints. \n",
    "#### THe cluster assignment i done with faiss, and the keyword extraction is done per cluster by taking top 100 words in each cluster and filtering them using a noun and dictionary filter.\n",
    "#### The final word representations was a bit unclear and needs better feature representations, which can give better words at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "superb-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: provide your solution here.\n",
    "# Convert headlines to TF-IDF vectors\n",
    "headlines = news_df.headline.tolist()\n",
    "headlines_preprocessed = list(map(preprocess_text, headlines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5df5ca3d-f83f-4252-8ced-145fd83a1a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209527, 1000)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.85, stop_words='english', use_idf=True,max_features=1000)\n",
    "tfidf_sparse_matrix = vectorizer.fit_transform(headlines_preprocessed)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# #Use TruncatedSVD to reduce dimensionality of the sparse TF-IDF matrix\n",
    "# n_components = 1000\n",
    "# svd = TruncatedSVD(n_components=n_components)\n",
    "# reduced_tfidf_matrix = svd.fit_transform(tfidf_sparse_matrix)\n",
    "\n",
    "tfidf_sparse_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86f5c747-8cd1-4db4-8ddc-c8b70a40b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the reduced TF-IDF matrix to float32 for FAISS\n",
    "#reduced_tfidf_matrix = np.asarray(reduced_tfidf_matrix, dtype=np.float32)\n",
    "tfidf_dense_matrix = tfidf_sparse_matrix.toarray()\n",
    "tfidf_sparse_matrix = np.asarray(tfidf_dense_matrix, dtype=np.float32)\n",
    "# Create a FAISS indexflatl2 for clustering\n",
    "dimensionality = tfidf_dense_matrix.shape[1]\n",
    "cluster_index = faiss.IndexFlatL2(dimensionality)\n",
    "\n",
    "# Initialize the k-means clustering in FAISS\n",
    "k = 25 # Number of clusters\n",
    "niter = 50 # Number of iterations\n",
    "faiss_clustering = faiss.Clustering(dimensionality, k)\n",
    "faiss_clustering.niter = niter\n",
    "\n",
    "# Train the clustering model\n",
    "#faiss_clustering.train(reduced_tfidf_matrix, cluster_index)\n",
    "faiss_clustering.train(tfidf_sparse_matrix, cluster_index)\n",
    "\n",
    "# Assign each headline to a cluster\n",
    "#_, cluster_assignments = cluster_index.search(reduced_tfidf_matrix, 1)\n",
    "_, cluster_assignments = cluster_index.search(tfidf_sparse_matrix, 1)\n",
    "cluster_assignments = cluster_assignments.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1bea3592-7920-4e4b-8597-703bfe37e464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19    188576\n",
       "5       2116\n",
       "1       2030\n",
       "23      1860\n",
       "21      1810\n",
       "11      1722\n",
       "14      1499\n",
       "10      1364\n",
       "3       1352\n",
       "7        972\n",
       "22       898\n",
       "8        787\n",
       "12       648\n",
       "24       635\n",
       "17       588\n",
       "15       553\n",
       "20       481\n",
       "4        429\n",
       "18       391\n",
       "2        383\n",
       "16       374\n",
       "9         37\n",
       "0         11\n",
       "6          6\n",
       "13         5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cluster_assignments).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "535a4176-8b23-4876-8f57-09e1f7b7f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# Get the set of English words\n",
    "english_words = set(words.words())\n",
    "\n",
    "def filter_keywords(keywords):\n",
    "    \"\"\"Filter keywords to retain only dictionary nouns.\"\"\"\n",
    "    # Tokenize and get part-of-speech tags\n",
    "    pos_tags = nltk.pos_tag(keywords)\n",
    "    \n",
    "    # Filter for nouns that are in the English dictionary\n",
    "    filtered_keywords = [word for word, pos in pos_tags if (word in english_words) and (pos in ['NN', 'NNS'])]\n",
    "    \n",
    "    return filtered_keywords\n",
    "\n",
    "def get_top_keywords(tfidf_matrix, cluster_assignments, cluster_number, top_n=100):\n",
    "    \"\"\"Get top N keywords for a given cluster.\"\"\"\n",
    "    cluster_data = tfidf_matrix[cluster_assignments == cluster_number]\n",
    "    # Sum TF-IDF vectors of all samples in the cluster\n",
    "    summed_cluster_tfidf = np.sum(cluster_data, axis=0)\n",
    "    # Get indices of top N keywords\n",
    "    top_indices = np.argsort(summed_cluster_tfidf)[-top_n:][::-1]\n",
    "    top_indices = np.array(top_indices).flatten()\n",
    "    # Get keywords\n",
    "    keywords = [feature_names[idx] for idx in top_indices]\n",
    "    # Filter the keywords to retain only dictionary nouns\n",
    "    filtered_keywords = filter_keywords(keywords)\n",
    "    return filtered_keywords[:top_n]\n",
    "\n",
    "cluster_representations = {}\n",
    "for cluster_num in range(25):\n",
    "    keywords = get_top_keywords(reduced_tfidf_matrix, cluster_assignments, cluster_num)\n",
    "    cluster_representations[cluster_num] = keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3d62c7ad-0494-4d79-8993-a8930d8dd02d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['kind',\n",
       "  'baby',\n",
       "  'loss',\n",
       "  'abortion',\n",
       "  'mean',\n",
       "  'meet',\n",
       "  'issue',\n",
       "  'joe',\n",
       "  'hero',\n",
       "  'season',\n",
       "  'journalist',\n",
       "  'lie',\n",
       "  'launch',\n",
       "  'study',\n",
       "  'law',\n",
       "  'record',\n",
       "  'mayor',\n",
       "  'egg',\n",
       "  'sale',\n",
       "  'restaurant',\n",
       "  'learn',\n",
       "  'fact',\n",
       "  'depression',\n",
       "  'person',\n",
       "  'job',\n",
       "  'man',\n",
       "  'pregnancy',\n",
       "  'pound',\n",
       "  'ban',\n",
       "  'royal',\n",
       "  'spending',\n",
       "  'soccer',\n",
       "  'evolution',\n",
       "  'future',\n",
       "  'hospital',\n",
       "  'drink',\n",
       "  'price',\n",
       "  'force',\n",
       "  'freedom',\n",
       "  'mind',\n",
       "  'holiday',\n",
       "  'scandal',\n",
       "  'pick',\n",
       "  'lawsuit',\n",
       "  'program',\n",
       "  'beauty',\n",
       "  'strike',\n",
       "  'drop',\n",
       "  'murder',\n",
       "  'party'],\n",
       " 1: ['abortion',\n",
       "  'animal',\n",
       "  'album',\n",
       "  'abuse',\n",
       "  'address',\n",
       "  'air',\n",
       "  'autism',\n",
       "  'art',\n",
       "  'battle',\n",
       "  'arrest',\n",
       "  'bank',\n",
       "  'advice',\n",
       "  'benefit',\n",
       "  'birthday',\n",
       "  'artist',\n",
       "  'award',\n",
       "  'ben',\n",
       "  'blood',\n",
       "  'actress',\n",
       "  'age',\n",
       "  'book',\n",
       "  'border',\n",
       "  'college',\n",
       "  'beat',\n",
       "  'choice',\n",
       "  'box',\n",
       "  'breast',\n",
       "  'director',\n",
       "  'celebrity',\n",
       "  'chief',\n",
       "  'brand',\n",
       "  'cup',\n",
       "  'career',\n",
       "  'body',\n",
       "  'chicken',\n",
       "  'face',\n",
       "  'city',\n",
       "  'destination',\n",
       "  'court',\n",
       "  'design',\n",
       "  'coffee',\n",
       "  'check',\n",
       "  'dog',\n",
       "  'credit',\n",
       "  'comment',\n",
       "  'blue',\n",
       "  'chocolate',\n",
       "  'lawsuit',\n",
       "  'lawmaker',\n",
       "  'celebrate',\n",
       "  'cause',\n",
       "  'beer',\n",
       "  'feature',\n",
       "  'divorce',\n",
       "  'guest',\n",
       "  'deal',\n",
       "  'experience',\n",
       "  'harry',\n",
       "  'giant',\n",
       "  'cop',\n",
       "  'equality',\n",
       "  'myth',\n",
       "  'flight',\n",
       "  'designer'],\n",
       " 2: ['abortion',\n",
       "  'peace',\n",
       "  'post',\n",
       "  'offer',\n",
       "  'result',\n",
       "  'month',\n",
       "  'parenthood',\n",
       "  'man',\n",
       "  'morning',\n",
       "  'price',\n",
       "  'president',\n",
       "  'place',\n",
       "  'parent',\n",
       "  'school',\n",
       "  'magazine',\n",
       "  'promise',\n",
       "  'network',\n",
       "  'office',\n",
       "  'response',\n",
       "  'kitchen',\n",
       "  'meeting',\n",
       "  'pregnancy',\n",
       "  'men',\n",
       "  'ring',\n",
       "  'jimmy',\n",
       "  'record',\n",
       "  'marijuana',\n",
       "  'power',\n",
       "  'holiday',\n",
       "  'rock',\n",
       "  'join',\n",
       "  'note',\n",
       "  'hope',\n",
       "  'science',\n",
       "  'kill',\n",
       "  'risk',\n",
       "  'learn',\n",
       "  'outfit',\n",
       "  'policy',\n",
       "  'pain',\n",
       "  'share',\n",
       "  'line',\n",
       "  'life',\n",
       "  'fox',\n",
       "  'summer',\n",
       "  'immigration'],\n",
       " 3: ['baby',\n",
       "  'attack',\n",
       "  'bank',\n",
       "  'attorney',\n",
       "  'bar',\n",
       "  'abortion',\n",
       "  'beauty',\n",
       "  'benefit',\n",
       "  'arrest',\n",
       "  'ask',\n",
       "  'album',\n",
       "  'body',\n",
       "  'air',\n",
       "  'aid',\n",
       "  'bomb',\n",
       "  'beer',\n",
       "  'book',\n",
       "  'blood',\n",
       "  'birth',\n",
       "  'cancer',\n",
       "  'breast',\n",
       "  'budget',\n",
       "  'company',\n",
       "  'card',\n",
       "  'age',\n",
       "  'cocktail',\n",
       "  'celebrate',\n",
       "  'business',\n",
       "  'breakfast',\n",
       "  'crisis',\n",
       "  'claim',\n",
       "  'allegation',\n",
       "  'box',\n",
       "  'charge',\n",
       "  'airport',\n",
       "  'destination',\n",
       "  'address',\n",
       "  'credit',\n",
       "  'board',\n",
       "  'coach',\n",
       "  'chicken',\n",
       "  'control',\n",
       "  'divorce',\n",
       "  'date',\n",
       "  'disease',\n",
       "  'drug',\n",
       "  'conversation',\n",
       "  'depression',\n",
       "  'election',\n",
       "  'football',\n",
       "  'candidate',\n",
       "  'equality',\n",
       "  'episode',\n",
       "  'change',\n",
       "  'goal',\n",
       "  'fix',\n",
       "  'color',\n",
       "  'group'],\n",
       " 4: ['face',\n",
       "  'driver',\n",
       "  'myth',\n",
       "  'family',\n",
       "  'model',\n",
       "  'office',\n",
       "  'depression',\n",
       "  'governor',\n",
       "  'drug',\n",
       "  'pound',\n",
       "  'magazine',\n",
       "  'summer',\n",
       "  'abortion',\n",
       "  'investigation',\n",
       "  'prison',\n",
       "  'team',\n",
       "  'view',\n",
       "  'group',\n",
       "  'talk',\n",
       "  'fall',\n",
       "  'pop',\n",
       "  'supreme',\n",
       "  'news',\n",
       "  'victory',\n",
       "  'kim',\n",
       "  'middle',\n",
       "  'strategy',\n",
       "  'teen',\n",
       "  'mother',\n",
       "  'episode',\n",
       "  'raise',\n",
       "  'site',\n",
       "  'today',\n",
       "  'scientist',\n",
       "  'dark',\n",
       "  'mind',\n",
       "  'book',\n",
       "  'penny',\n",
       "  'vote',\n",
       "  'sue',\n",
       "  'day',\n",
       "  'character',\n",
       "  'road',\n",
       "  'session',\n",
       "  'study',\n",
       "  'history',\n",
       "  'challenge',\n",
       "  'movement',\n",
       "  'fake',\n",
       "  'habit'],\n",
       " 5: ['abuse',\n",
       "  'attack',\n",
       "  'abortion',\n",
       "  'ban',\n",
       "  'avoid',\n",
       "  'beach',\n",
       "  'blame',\n",
       "  'birthday',\n",
       "  'beer',\n",
       "  'award',\n",
       "  'artist',\n",
       "  'birth',\n",
       "  'actor',\n",
       "  'advice',\n",
       "  'air',\n",
       "  'battle',\n",
       "  'address',\n",
       "  'ad',\n",
       "  'budget',\n",
       "  'assault',\n",
       "  'chance',\n",
       "  'choice',\n",
       "  'art',\n",
       "  'debut',\n",
       "  'death',\n",
       "  'blue',\n",
       "  'bank',\n",
       "  'campaign',\n",
       "  'date',\n",
       "  'daughter',\n",
       "  'box',\n",
       "  'benefit',\n",
       "  'debate',\n",
       "  'child',\n",
       "  'design',\n",
       "  'community',\n",
       "  'blast',\n",
       "  'cup',\n",
       "  'business',\n",
       "  'earth',\n",
       "  'cat',\n",
       "  'build',\n",
       "  'country',\n",
       "  'case',\n",
       "  'age',\n",
       "  'change',\n",
       "  'finding',\n",
       "  'eat',\n",
       "  'congress',\n",
       "  'dinner',\n",
       "  'couple',\n",
       "  'health',\n",
       "  'experience',\n",
       "  'cut',\n",
       "  'city',\n",
       "  'coffee',\n",
       "  'comment',\n",
       "  'drop',\n",
       "  'crisis'],\n",
       " 6: ['birth',\n",
       "  'abortion',\n",
       "  'food',\n",
       "  'church',\n",
       "  'crash',\n",
       "  'chief',\n",
       "  'fun',\n",
       "  'couple',\n",
       "  'cover',\n",
       "  'fix',\n",
       "  'charge',\n",
       "  'family',\n",
       "  'hurricane',\n",
       "  'company',\n",
       "  'care',\n",
       "  'budget',\n",
       "  'heart',\n",
       "  'choice',\n",
       "  'football',\n",
       "  'fail',\n",
       "  'eye',\n",
       "  'governor',\n",
       "  'gold',\n",
       "  'daughter',\n",
       "  'carpet',\n",
       "  'breast',\n",
       "  'dance',\n",
       "  'bomb',\n",
       "  'holiday',\n",
       "  'cause',\n",
       "  'climate',\n",
       "  'feature',\n",
       "  'interview',\n",
       "  'force',\n",
       "  'control',\n",
       "  'craft',\n",
       "  'festival',\n",
       "  'gun',\n",
       "  'disease',\n",
       "  'claim',\n",
       "  'actor',\n",
       "  'joy',\n",
       "  'experience',\n",
       "  'hotel',\n",
       "  'coffee',\n",
       "  'beach',\n",
       "  'fighting',\n",
       "  'expert',\n",
       "  'judge',\n",
       "  'brand',\n",
       "  'business',\n",
       "  'blood',\n",
       "  'job',\n",
       "  'bar',\n",
       "  'depression',\n",
       "  'dad',\n",
       "  'body',\n",
       "  'employee'],\n",
       " 7: ['board',\n",
       "  'body',\n",
       "  'abortion',\n",
       "  'blame',\n",
       "  'birth',\n",
       "  'car',\n",
       "  'bowl',\n",
       "  'blood',\n",
       "  'border',\n",
       "  'card',\n",
       "  'break',\n",
       "  'campaign',\n",
       "  'bride',\n",
       "  'bomb',\n",
       "  'expert',\n",
       "  'brother',\n",
       "  'care',\n",
       "  'globe',\n",
       "  'attorney',\n",
       "  'harry',\n",
       "  'brand',\n",
       "  'beauty',\n",
       "  'eye',\n",
       "  'bank',\n",
       "  'baby',\n",
       "  'conversation',\n",
       "  'beer',\n",
       "  'country',\n",
       "  'goal',\n",
       "  'answer',\n",
       "  'change',\n",
       "  'film',\n",
       "  'check',\n",
       "  'fact',\n",
       "  'budget',\n",
       "  'habit',\n",
       "  'decision',\n",
       "  'couple',\n",
       "  'fear',\n",
       "  'cause',\n",
       "  'hand',\n",
       "  'charge',\n",
       "  'crash',\n",
       "  'benefit',\n",
       "  'chicken',\n",
       "  'disease',\n",
       "  'head',\n",
       "  'dear',\n",
       "  'choice',\n",
       "  'help',\n",
       "  'church',\n",
       "  'life',\n",
       "  'homeless',\n",
       "  'fall',\n",
       "  'flight'],\n",
       " 8: ['center',\n",
       "  'chance',\n",
       "  'check',\n",
       "  'abuse',\n",
       "  'care',\n",
       "  'character',\n",
       "  'china',\n",
       "  'abortion',\n",
       "  'child',\n",
       "  'carpet',\n",
       "  'city',\n",
       "  'cancer',\n",
       "  'chocolate',\n",
       "  'account',\n",
       "  'activist',\n",
       "  'comment',\n",
       "  'box',\n",
       "  'book',\n",
       "  'church',\n",
       "  'couple',\n",
       "  'bar',\n",
       "  'coffee',\n",
       "  'autism',\n",
       "  'crazy',\n",
       "  'action',\n",
       "  'demand',\n",
       "  'coach',\n",
       "  'block',\n",
       "  'bomb',\n",
       "  'company',\n",
       "  'boost',\n",
       "  'birthday',\n",
       "  'blame',\n",
       "  'attorney',\n",
       "  'arrest',\n",
       "  'bring',\n",
       "  'defense',\n",
       "  'apple',\n",
       "  'death',\n",
       "  'bank',\n",
       "  'disease',\n",
       "  'equality',\n",
       "  'beer',\n",
       "  'address',\n",
       "  'community',\n",
       "  'effect',\n",
       "  'job',\n",
       "  'earth',\n",
       "  'egg',\n",
       "  'benefit',\n",
       "  'hold',\n",
       "  'date',\n",
       "  'jeff',\n",
       "  'animal',\n",
       "  'director',\n",
       "  'designer',\n",
       "  'bowl',\n",
       "  'eye'],\n",
       " 9: ['state',\n",
       "  'stage',\n",
       "  'team',\n",
       "  'lawsuit',\n",
       "  'lawmaker',\n",
       "  'sport',\n",
       "  'street',\n",
       "  'labor',\n",
       "  'lie',\n",
       "  'sign',\n",
       "  'park',\n",
       "  'joy',\n",
       "  'kill',\n",
       "  'survivor',\n",
       "  'session',\n",
       "  'rule',\n",
       "  'star',\n",
       "  'teacher',\n",
       "  'secretary',\n",
       "  'refugee',\n",
       "  'racist',\n",
       "  'racism',\n",
       "  'security',\n",
       "  'queen',\n",
       "  'technology',\n",
       "  'start',\n",
       "  'push',\n",
       "  'senator',\n",
       "  'spending',\n",
       "  'relationship',\n",
       "  'trump',\n",
       "  'rate',\n",
       "  'tip',\n",
       "  'challenge',\n",
       "  'rise',\n",
       "  'judge',\n",
       "  'resolution',\n",
       "  'abortion',\n",
       "  'moment',\n",
       "  'person',\n",
       "  'evolution',\n",
       "  'view',\n",
       "  'today',\n",
       "  'check',\n",
       "  'share',\n",
       "  'mistake',\n",
       "  'marijuana',\n",
       "  'reveal',\n",
       "  'urge',\n",
       "  'tax',\n",
       "  'post',\n",
       "  'vacation',\n",
       "  'joke',\n",
       "  'health',\n",
       "  'season',\n",
       "  'outfit'],\n",
       " 10: ['attack',\n",
       "  'art',\n",
       "  'abortion',\n",
       "  'abuse',\n",
       "  'baby',\n",
       "  'apple',\n",
       "  'airport',\n",
       "  'blame',\n",
       "  'birthday',\n",
       "  'actress',\n",
       "  'air',\n",
       "  'beach',\n",
       "  'ad',\n",
       "  'administration',\n",
       "  'change',\n",
       "  'brain',\n",
       "  'body',\n",
       "  'book',\n",
       "  'blood',\n",
       "  'choice',\n",
       "  'claim',\n",
       "  'beauty',\n",
       "  'climate',\n",
       "  'car',\n",
       "  'age',\n",
       "  'career',\n",
       "  'cost',\n",
       "  'budget',\n",
       "  'benefit',\n",
       "  'coffee',\n",
       "  'celebrity',\n",
       "  'brand',\n",
       "  'deal',\n",
       "  'child',\n",
       "  'bush',\n",
       "  'control',\n",
       "  'gay',\n",
       "  'capture',\n",
       "  'dog',\n",
       "  'college',\n",
       "  'credit',\n",
       "  'game',\n",
       "  'evolution',\n",
       "  'line',\n",
       "  'disaster',\n",
       "  'eye',\n",
       "  'death',\n",
       "  'harassment',\n",
       "  'cut'],\n",
       " 11: ['abortion',\n",
       "  'album',\n",
       "  'actress',\n",
       "  'affect',\n",
       "  'air',\n",
       "  'art',\n",
       "  'date',\n",
       "  'airport',\n",
       "  'dad',\n",
       "  'cut',\n",
       "  'dark',\n",
       "  'democrat',\n",
       "  'benefit',\n",
       "  'design',\n",
       "  'department',\n",
       "  'apple',\n",
       "  'assault',\n",
       "  'court',\n",
       "  'control',\n",
       "  'budget',\n",
       "  'adult',\n",
       "  'election',\n",
       "  'effect',\n",
       "  'debate',\n",
       "  'fox',\n",
       "  'freedom',\n",
       "  'chocolate',\n",
       "  'education',\n",
       "  'food',\n",
       "  'address',\n",
       "  'data',\n",
       "  'doctor',\n",
       "  'coffee',\n",
       "  'breast',\n",
       "  'disaster',\n",
       "  'country',\n",
       "  'beauty',\n",
       "  'award',\n",
       "  'blast',\n",
       "  'board',\n",
       "  'episode',\n",
       "  'career',\n",
       "  'congress',\n",
       "  'day',\n",
       "  'destination',\n",
       "  'box',\n",
       "  'festival',\n",
       "  'designer',\n",
       "  'comment',\n",
       "  'age',\n",
       "  'change',\n",
       "  'depression',\n",
       "  'attorney',\n",
       "  'charge',\n",
       "  'force',\n",
       "  'birthday',\n",
       "  'experience',\n",
       "  'challenge'],\n",
       " 12: ['chocolate',\n",
       "  'decision',\n",
       "  'clean',\n",
       "  'character',\n",
       "  'charge',\n",
       "  'check',\n",
       "  'defense',\n",
       "  'depression',\n",
       "  'divorce',\n",
       "  'abortion',\n",
       "  'cocktail',\n",
       "  'company',\n",
       "  'create',\n",
       "  'daughter',\n",
       "  'cause',\n",
       "  'design',\n",
       "  'card',\n",
       "  'director',\n",
       "  'church',\n",
       "  'demand',\n",
       "  'country',\n",
       "  'dance',\n",
       "  'disease',\n",
       "  'case',\n",
       "  'coach',\n",
       "  'earth',\n",
       "  'control',\n",
       "  'debut',\n",
       "  'bring',\n",
       "  'egg',\n",
       "  'effect',\n",
       "  'comment',\n",
       "  'energy',\n",
       "  'arrest',\n",
       "  'account',\n",
       "  'body',\n",
       "  'cover',\n",
       "  'answer',\n",
       "  'airport',\n",
       "  'credit',\n",
       "  'beer',\n",
       "  'battle',\n",
       "  'birthday',\n",
       "  'data',\n",
       "  'capture',\n",
       "  'blast',\n",
       "  'beach',\n",
       "  'doctor',\n",
       "  'letter',\n",
       "  'beauty',\n",
       "  'brain',\n",
       "  'food',\n",
       "  'book',\n",
       "  'actor',\n",
       "  'expert',\n",
       "  'bowl'],\n",
       " 13: ['abortion',\n",
       "  'issue',\n",
       "  'culture',\n",
       "  'park',\n",
       "  'news',\n",
       "  'music',\n",
       "  'morning',\n",
       "  'murder',\n",
       "  'jimmy',\n",
       "  'crisis',\n",
       "  'pay',\n",
       "  'series',\n",
       "  'gift',\n",
       "  'end',\n",
       "  'joke',\n",
       "  'month',\n",
       "  'senator',\n",
       "  'relationship',\n",
       "  'ex',\n",
       "  'skin',\n",
       "  'senate',\n",
       "  'rape',\n",
       "  'launch',\n",
       "  'look',\n",
       "  'target',\n",
       "  'government',\n",
       "  'event',\n",
       "  'hero',\n",
       "  'people',\n",
       "  'rise',\n",
       "  'sleep',\n",
       "  'fix',\n",
       "  'cool',\n",
       "  'restaurant',\n",
       "  'try',\n",
       "  'costume',\n",
       "  'game',\n",
       "  'politics',\n",
       "  'defense',\n",
       "  'mental',\n",
       "  'hair',\n",
       "  'crazy',\n",
       "  'film',\n",
       "  'hall',\n",
       "  'stress',\n",
       "  'cover',\n",
       "  'legacy'],\n",
       " 14: ['artist',\n",
       "  'blame',\n",
       "  'abortion',\n",
       "  'apple',\n",
       "  'award',\n",
       "  'bomb',\n",
       "  'beach',\n",
       "  'comment',\n",
       "  'community',\n",
       "  'brand',\n",
       "  'boost',\n",
       "  'advice',\n",
       "  'cocktail',\n",
       "  'brother',\n",
       "  'ad',\n",
       "  'allegation',\n",
       "  'amy',\n",
       "  'board',\n",
       "  'congress',\n",
       "  'company',\n",
       "  'album',\n",
       "  'cool',\n",
       "  'act',\n",
       "  'country',\n",
       "  'court',\n",
       "  'challenge',\n",
       "  'control',\n",
       "  'border',\n",
       "  'assault',\n",
       "  'answer',\n",
       "  'candidate',\n",
       "  'carpet',\n",
       "  'care',\n",
       "  'career',\n",
       "  'bowl',\n",
       "  'cat',\n",
       "  'card',\n",
       "  'cop',\n",
       "  'bar',\n",
       "  'college',\n",
       "  'crime',\n",
       "  'chief',\n",
       "  'drink',\n",
       "  'ben',\n",
       "  'deal',\n",
       "  'debut',\n",
       "  'bush',\n",
       "  'change',\n",
       "  'demand',\n",
       "  'day',\n",
       "  'daughter',\n",
       "  'gold',\n",
       "  'director',\n",
       "  'crisis',\n",
       "  'claim',\n",
       "  'democrat',\n",
       "  'center'],\n",
       " 15: ['mind',\n",
       "  'meditation',\n",
       "  'morning',\n",
       "  'mother',\n",
       "  'movie',\n",
       "  'abortion',\n",
       "  'nation',\n",
       "  'airport',\n",
       "  'medium',\n",
       "  'message',\n",
       "  'news',\n",
       "  'cost',\n",
       "  'country',\n",
       "  'cup',\n",
       "  'island',\n",
       "  'judge',\n",
       "  'jeff',\n",
       "  'date',\n",
       "  'child',\n",
       "  'loss',\n",
       "  'men',\n",
       "  'chief',\n",
       "  'need',\n",
       "  'change',\n",
       "  'month',\n",
       "  'lawmaker',\n",
       "  'advice',\n",
       "  'mean',\n",
       "  'carpet',\n",
       "  'result',\n",
       "  'house',\n",
       "  'charge',\n",
       "  'race',\n",
       "  'home',\n",
       "  'kind',\n",
       "  'cool',\n",
       "  'offer',\n",
       "  'claim',\n",
       "  'journalist',\n",
       "  'create',\n",
       "  'safety',\n",
       "  'congress',\n",
       "  'attack',\n",
       "  'cut',\n",
       "  'center',\n",
       "  'dog',\n",
       "  'dress',\n",
       "  'labor',\n",
       "  'immigration'],\n",
       " 16: ['list',\n",
       "  'life',\n",
       "  'love',\n",
       "  'marriage',\n",
       "  'lot',\n",
       "  'abortion',\n",
       "  'light',\n",
       "  'man',\n",
       "  'lead',\n",
       "  'line',\n",
       "  'interview',\n",
       "  'lawmaker',\n",
       "  'minute',\n",
       "  'let',\n",
       "  'magazine',\n",
       "  'mind',\n",
       "  'matter',\n",
       "  'journalist',\n",
       "  'joe',\n",
       "  'album',\n",
       "  'hurricane',\n",
       "  'member',\n",
       "  'air',\n",
       "  'note',\n",
       "  'hour',\n",
       "  'mass',\n",
       "  'people',\n",
       "  'event',\n",
       "  'product',\n",
       "  'film',\n",
       "  'energy',\n",
       "  'network',\n",
       "  'kim',\n",
       "  'news',\n",
       "  'feature',\n",
       "  'post',\n",
       "  'money',\n",
       "  'officer',\n",
       "  'crime',\n",
       "  'president',\n",
       "  'harassment',\n",
       "  'hate',\n",
       "  'reason',\n",
       "  'help',\n",
       "  'pound',\n",
       "  'head',\n",
       "  'protest',\n",
       "  'right',\n",
       "  'safety',\n",
       "  'employee',\n",
       "  'habit'],\n",
       " 17: ['country',\n",
       "  'crime',\n",
       "  'college',\n",
       "  'dance',\n",
       "  'cop',\n",
       "  'data',\n",
       "  'culture',\n",
       "  'abortion',\n",
       "  'dark',\n",
       "  'community',\n",
       "  'abuse',\n",
       "  'debut',\n",
       "  'engagement',\n",
       "  'church',\n",
       "  'dinner',\n",
       "  'court',\n",
       "  'deal',\n",
       "  'cooking',\n",
       "  'baby',\n",
       "  'design',\n",
       "  'coach',\n",
       "  'decision',\n",
       "  'attack',\n",
       "  'dog',\n",
       "  'chance',\n",
       "  'career',\n",
       "  'center',\n",
       "  'capture',\n",
       "  'autism',\n",
       "  'cause',\n",
       "  'demand',\n",
       "  'class',\n",
       "  'cake',\n",
       "  'box',\n",
       "  'director',\n",
       "  'driver',\n",
       "  'birthday',\n",
       "  'department',\n",
       "  'assault',\n",
       "  'blast',\n",
       "  'daughter',\n",
       "  'beer',\n",
       "  'card',\n",
       "  'boy',\n",
       "  'choice',\n",
       "  'child',\n",
       "  'advice',\n",
       "  'beauty',\n",
       "  'bowl',\n",
       "  'exercise',\n",
       "  'gift',\n",
       "  'disaster',\n",
       "  'album',\n",
       "  'allegation',\n",
       "  'energy',\n",
       "  'campaign',\n",
       "  'expert',\n",
       "  'airport'],\n",
       " 18: ['network',\n",
       "  'refugee',\n",
       "  'matter',\n",
       "  'meditation',\n",
       "  'loss',\n",
       "  'racist',\n",
       "  'march',\n",
       "  'look',\n",
       "  'rise',\n",
       "  'prison',\n",
       "  'reporter',\n",
       "  'raise',\n",
       "  'price',\n",
       "  'president',\n",
       "  'peace',\n",
       "  'event',\n",
       "  'abortion',\n",
       "  'letter',\n",
       "  'men',\n",
       "  'survey',\n",
       "  'nail',\n",
       "  'money',\n",
       "  'kind',\n",
       "  'program',\n",
       "  'pop',\n",
       "  'police',\n",
       "  'scandal',\n",
       "  'note',\n",
       "  'nation',\n",
       "  'election',\n",
       "  'rock',\n",
       "  'nominee',\n",
       "  'mass',\n",
       "  'mental',\n",
       "  'minute',\n",
       "  'divorce',\n",
       "  'month',\n",
       "  'line',\n",
       "  'medium',\n",
       "  'pay',\n",
       "  'project',\n",
       "  'drop',\n",
       "  'employee',\n",
       "  'air',\n",
       "  'film',\n",
       "  'player',\n",
       "  'speech',\n",
       "  'picture',\n",
       "  'research',\n",
       "  'depression',\n",
       "  'mark'],\n",
       " 19: ['abortion',\n",
       "  'abuse',\n",
       "  'beach',\n",
       "  'air',\n",
       "  'arrest',\n",
       "  'block',\n",
       "  'advice',\n",
       "  'bring',\n",
       "  'bowl',\n",
       "  'business',\n",
       "  'ask',\n",
       "  'album',\n",
       "  'airport',\n",
       "  'blood',\n",
       "  'bank',\n",
       "  'affect',\n",
       "  'class',\n",
       "  'day',\n",
       "  'bar',\n",
       "  'birthday',\n",
       "  'case',\n",
       "  'cost',\n",
       "  'capture',\n",
       "  'battle',\n",
       "  'book',\n",
       "  'character',\n",
       "  'apple',\n",
       "  'candidate',\n",
       "  'cover',\n",
       "  'blast',\n",
       "  'drink',\n",
       "  'joke',\n",
       "  'hate',\n",
       "  'dress',\n",
       "  'cause',\n",
       "  'create',\n",
       "  'mark',\n",
       "  'baby',\n",
       "  'china',\n",
       "  'depression',\n",
       "  'crime',\n",
       "  'hour',\n",
       "  'end',\n",
       "  'church',\n",
       "  'community',\n",
       "  'college',\n",
       "  'chicken',\n",
       "  'crisis',\n",
       "  'fashion',\n",
       "  'justice',\n",
       "  'family',\n",
       "  'election',\n",
       "  'gay',\n",
       "  'company',\n",
       "  'cute',\n",
       "  'democrat',\n",
       "  'ex',\n",
       "  'birth'],\n",
       " 20: ['investigation',\n",
       "  'jimmy',\n",
       "  'holiday',\n",
       "  'law',\n",
       "  'career',\n",
       "  'la',\n",
       "  'abortion',\n",
       "  'hotel',\n",
       "  'business',\n",
       "  'land',\n",
       "  'lawmaker',\n",
       "  'kitchen',\n",
       "  'leader',\n",
       "  'brother',\n",
       "  'abuse',\n",
       "  'kill',\n",
       "  'journalist',\n",
       "  'friend',\n",
       "  'homeless',\n",
       "  'head',\n",
       "  'hurricane',\n",
       "  'future',\n",
       "  'cat',\n",
       "  'legacy',\n",
       "  'gold',\n",
       "  'husband',\n",
       "  'demand',\n",
       "  'ice',\n",
       "  'guy',\n",
       "  'loss',\n",
       "  'dear',\n",
       "  'hand',\n",
       "  'let',\n",
       "  'effect',\n",
       "  'celebrity',\n",
       "  'hour',\n",
       "  'movement',\n",
       "  'fox',\n",
       "  'celebrate',\n",
       "  'chance',\n",
       "  'education',\n",
       "  'blue',\n",
       "  'coffee',\n",
       "  'control',\n",
       "  'king',\n",
       "  'driver',\n",
       "  'march',\n",
       "  'model',\n",
       "  'arrest',\n",
       "  'air',\n",
       "  'gift'],\n",
       " 21: ['autism',\n",
       "  'abortion',\n",
       "  'ad',\n",
       "  'arrest',\n",
       "  'abuse',\n",
       "  'battle',\n",
       "  'act',\n",
       "  'activist',\n",
       "  'answer',\n",
       "  'administration',\n",
       "  'beer',\n",
       "  'ben',\n",
       "  'air',\n",
       "  'care',\n",
       "  'beat',\n",
       "  'advice',\n",
       "  'art',\n",
       "  'cancer',\n",
       "  'budget',\n",
       "  'equality',\n",
       "  'center',\n",
       "  'album',\n",
       "  'bank',\n",
       "  'congressman',\n",
       "  'breast',\n",
       "  'brain',\n",
       "  'breakfast',\n",
       "  'board',\n",
       "  'director',\n",
       "  'choice',\n",
       "  'image',\n",
       "  'control',\n",
       "  'cup',\n",
       "  'building',\n",
       "  'car',\n",
       "  'charge',\n",
       "  'design',\n",
       "  'date',\n",
       "  'college',\n",
       "  'road',\n",
       "  'shoe',\n",
       "  'card',\n",
       "  'eat',\n",
       "  'end',\n",
       "  'hall',\n",
       "  'resolution',\n",
       "  'dream',\n",
       "  'bush',\n",
       "  'cost',\n",
       "  'drop',\n",
       "  'culture',\n",
       "  'death',\n",
       "  'credit',\n",
       "  'help',\n",
       "  'green'],\n",
       " 22: ['costume',\n",
       "  'company',\n",
       "  'couple',\n",
       "  'abortion',\n",
       "  'choice',\n",
       "  'coach',\n",
       "  'chocolate',\n",
       "  'air',\n",
       "  'crisis',\n",
       "  'cup',\n",
       "  'dance',\n",
       "  'ask',\n",
       "  'cocktail',\n",
       "  'breakfast',\n",
       "  'come',\n",
       "  'attack',\n",
       "  'border',\n",
       "  'coffee',\n",
       "  'answer',\n",
       "  'cut',\n",
       "  'bowl',\n",
       "  'department',\n",
       "  'designer',\n",
       "  'benefit',\n",
       "  'color',\n",
       "  'boost',\n",
       "  'blue',\n",
       "  'ban',\n",
       "  'credit',\n",
       "  'carpet',\n",
       "  'blast',\n",
       "  'debut',\n",
       "  'administration',\n",
       "  'day',\n",
       "  'decision',\n",
       "  'autism',\n",
       "  'date',\n",
       "  'attorney',\n",
       "  'box',\n",
       "  'celebrity',\n",
       "  'expert',\n",
       "  'dark',\n",
       "  'candidate',\n",
       "  'happiness',\n",
       "  'beer',\n",
       "  'bar',\n",
       "  'center',\n",
       "  'event',\n",
       "  'beauty',\n",
       "  'act',\n",
       "  'claim',\n",
       "  'airport',\n",
       "  'bomb',\n",
       "  'cat',\n",
       "  'card'],\n",
       " 23: ['answer',\n",
       "  'abortion',\n",
       "  'attorney',\n",
       "  'air',\n",
       "  'age',\n",
       "  'baby',\n",
       "  'bar',\n",
       "  'airport',\n",
       "  'birthday',\n",
       "  'advice',\n",
       "  'building',\n",
       "  'brain',\n",
       "  'conversation',\n",
       "  'crazy',\n",
       "  'country',\n",
       "  'bank',\n",
       "  'costume',\n",
       "  'challenge',\n",
       "  'care',\n",
       "  'career',\n",
       "  'cancer',\n",
       "  'birth',\n",
       "  'chance',\n",
       "  'border',\n",
       "  'breast',\n",
       "  'block',\n",
       "  'habit',\n",
       "  'cop',\n",
       "  'deal',\n",
       "  'court',\n",
       "  'brother',\n",
       "  'day',\n",
       "  'fact',\n",
       "  'designer',\n",
       "  'flight',\n",
       "  'breakfast',\n",
       "  'disaster',\n",
       "  'card',\n",
       "  'host',\n",
       "  'home',\n",
       "  'cost',\n",
       "  'boost',\n",
       "  'capture',\n",
       "  'community',\n",
       "  'heart',\n",
       "  'harry',\n",
       "  'budget',\n",
       "  'gold',\n",
       "  'feature',\n",
       "  'college',\n",
       "  'hotel',\n",
       "  'husband'],\n",
       " 24: ['history',\n",
       "  'harry',\n",
       "  'hill',\n",
       "  'court',\n",
       "  'head',\n",
       "  'abortion',\n",
       "  'fat',\n",
       "  'festival',\n",
       "  'dress',\n",
       "  'case',\n",
       "  'event',\n",
       "  'home',\n",
       "  'award',\n",
       "  'earth',\n",
       "  'happiness',\n",
       "  'bowl',\n",
       "  'celebrate',\n",
       "  'fun',\n",
       "  'employee',\n",
       "  'depression',\n",
       "  'success',\n",
       "  'murder',\n",
       "  'create',\n",
       "  'hotel',\n",
       "  'hope',\n",
       "  'daughter',\n",
       "  'south',\n",
       "  'department',\n",
       "  'gun',\n",
       "  'director',\n",
       "  'doctor',\n",
       "  'ice',\n",
       "  'nation',\n",
       "  'coach',\n",
       "  'mind',\n",
       "  'conversation',\n",
       "  'scandal',\n",
       "  'list',\n",
       "  'join',\n",
       "  'state',\n",
       "  'target',\n",
       "  'student',\n",
       "  'season',\n",
       "  'gold',\n",
       "  'suicide',\n",
       "  'answer',\n",
       "  'justice',\n",
       "  'couple',\n",
       "  'storm',\n",
       "  'street',\n",
       "  'cost',\n",
       "  'challenge']}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-heating",
   "metadata": {},
   "source": [
    "## 2. Detecting complex query operations in natural language questions \n",
    "\n",
    "*You are building a pipeline that translates user-questions in corresponding SQL queries. As part of this pipeline, you need to build a classifier that detects complex query operations in a question.*\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We provide an excerpt of the [Spider dataset](https://yale-lily.github.io/spider) containing around 1k pairs of natural language questions and their corresponding SQL query. It contains the following attributes:\n",
    "\n",
    "- `question`: the natural language question.\n",
    "- `query`: the SQL query corresponding to the question.\n",
    "- `col_names`: description of the content of the columns\n",
    "- `col_names_original`: names of the columns in the database schema\n",
    "- `has_join`: if the query contains a join.\n",
    "- `has_groupby`: if the query contains a \"group by\" operation.\n",
    "- `has_orderby`: if the query contains an \"order by\" operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18f752c-aedc-4eb8-ad6e-b94ba5306422",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "1) Choose **one** of the \"join\", \"group by\" or \"order by\" operators, and build a binary classifier that, given a natural language question, detects if the corresponding query will use this operator. You may NOT use the \"query\" field. \n",
    "\n",
    "> We expect you to explain and show that your solution works as expected (e.g., through metrics on a test dataset)\n",
    "\n",
    "2) Describe in details how you would proceed if you had to identify which column from the schema (provided in the `col_names_original` in the dataset) needs to be put under the \"group by\" or \"order by\" statement. You can implement your solution but it is not mandatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "grateful-invasion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>query</th>\n",
       "      <th>col_names</th>\n",
       "      <th>col_names_original</th>\n",
       "      <th>has_join</th>\n",
       "      <th>has_groupby</th>\n",
       "      <th>has_orderby</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In which year were most departments established?</td>\n",
       "      <td>SELECT creation FROM department GROUP BY creat...</td>\n",
       "      <td>[*, department id, name, creation, ranking, bu...</td>\n",
       "      <td>[*, Department_ID, Name, Creation, Ranking, Bu...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>List the official name and status of the city ...</td>\n",
       "      <td>SELECT Official_Name ,  Status FROM city ORDER...</td>\n",
       "      <td>[*, city id, official name, status, area km 2,...</td>\n",
       "      <td>[*, City_ID, Official_Name, Status, Area_km_2,...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the official name and status of the ci...</td>\n",
       "      <td>SELECT Official_Name ,  Status FROM city ORDER...</td>\n",
       "      <td>[*, city id, official name, status, area km 2,...</td>\n",
       "      <td>[*, City_ID, Official_Name, Status, Area_km_2,...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Show the status of the city that has hosted th...</td>\n",
       "      <td>SELECT T1.Status FROM city AS T1 JOIN farm_com...</td>\n",
       "      <td>[*, city id, official name, status, area km 2,...</td>\n",
       "      <td>[*, City_ID, Official_Name, Status, Area_km_2,...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the status of the city that has hosted...</td>\n",
       "      <td>SELECT T1.Status FROM city AS T1 JOIN farm_com...</td>\n",
       "      <td>[*, city id, official name, status, area km 2,...</td>\n",
       "      <td>[*, City_ID, Official_Name, Status, Area_km_2,...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0   In which year were most departments established?   \n",
       "1  List the official name and status of the city ...   \n",
       "2  What is the official name and status of the ci...   \n",
       "3  Show the status of the city that has hosted th...   \n",
       "4  What is the status of the city that has hosted...   \n",
       "\n",
       "                                               query  \\\n",
       "0  SELECT creation FROM department GROUP BY creat...   \n",
       "1  SELECT Official_Name ,  Status FROM city ORDER...   \n",
       "2  SELECT Official_Name ,  Status FROM city ORDER...   \n",
       "3  SELECT T1.Status FROM city AS T1 JOIN farm_com...   \n",
       "4  SELECT T1.Status FROM city AS T1 JOIN farm_com...   \n",
       "\n",
       "                                           col_names  \\\n",
       "0  [*, department id, name, creation, ranking, bu...   \n",
       "1  [*, city id, official name, status, area km 2,...   \n",
       "2  [*, city id, official name, status, area km 2,...   \n",
       "3  [*, city id, official name, status, area km 2,...   \n",
       "4  [*, city id, official name, status, area km 2,...   \n",
       "\n",
       "                                  col_names_original  has_join  has_groupby  \\\n",
       "0  [*, Department_ID, Name, Creation, Ranking, Bu...     False         True   \n",
       "1  [*, City_ID, Official_Name, Status, Area_km_2,...     False        False   \n",
       "2  [*, City_ID, Official_Name, Status, Area_km_2,...     False        False   \n",
       "3  [*, City_ID, Official_Name, Status, Area_km_2,...      True         True   \n",
       "4  [*, City_ID, Official_Name, Status, Area_km_2,...      True         True   \n",
       "\n",
       "   has_orderby  \n",
       "0         True  \n",
       "1         True  \n",
       "2         True  \n",
       "3         True  \n",
       "4         True  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fname =\"./data/queries.json\"\n",
    "queries_df = pd.read_json(fname)\n",
    "queries_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2cee9-4f99-4786-8059-ccb0b43d7634",
   "metadata": {},
   "source": [
    "#### For the binary classification I use an XGBOOST model and we get an accuracy of 89%. As the categories are not very imbalanced category can be a valid metric to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f8f4b604-5435-4c83-8e88-11f0b9088cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((904, 898), (227, 898))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the dataset into training and test sets\n",
    "train_df, test_df = train_test_split(queries_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.85, max_features=1000,ngram_range=(1,1))\n",
    "\n",
    "# Fit and transform the questions in the training set\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['question'])\n",
    "\n",
    "# Transform the questions in the test set\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['question'])\n",
    "\n",
    "# Extract labels for training and test set\n",
    "y_train = train_df['has_groupby']\n",
    "y_test = test_df['has_groupby']\n",
    "\n",
    "X_train_tfidf.shape, X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "74f4c550-5765-4821-a698-9da480fc69d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      0.76      0.85        94\n",
      "        True       0.85      0.98      0.91       133\n",
      "\n",
      "    accuracy                           0.89       227\n",
      "   macro avg       0.90      0.87      0.88       227\n",
      "weighted avg       0.90      0.89      0.88       227\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Logistic Regression model\n",
    "clf = LogisticRegression(random_state=42, max_iter=1500)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "287f9350-a435-47f3-b7a4-9d74deb4e3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8773382173382174"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Perform 10-fold cross-validation\n",
    "# cv_scores = cross_val_score(clf, X_train_tfidf, y_train, cv=10, scoring=\"accuracy\")\n",
    "\n",
    "# cv_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-complement",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Detail solution for the column identification for group by and order by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e81c277e-09ff-4401-89a4-27dd8b1146ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Using traditional heuristic based techniques we could identify aggreggation keywords like \"each\", \"per\", \"total\" and oredring keywords like \"sorted\", \"descending\", etc. \\nThen we can check if any of the original col names occur in the questions. We can try entity linking with aggregation and ordering keywords to to try and ascertain the solution. \\nThis solution is very flimsy as it needs a lot of manual annotation and word maintainability.\\n2. Ideally this problem can be solved by  text to text transfer where we take a model like T5 and finetune it in a pattern like \\n\"Question + original sql columns[\"A\",\"B\",\"C\"] >> Group_by B order by C\" . As T5 is a language model it cann generalize to other questions also.\\n3. Another method would be to gather a lot of text to sql datasets and train a T5 based model from scratch. The finetuning can be done with step 2 which will help it generalize better.\\nThe dataset can be augmented by doing question generation from a second model which should be verified by human annotators.\\n3. The best possible current way is to use any sort of instruction tuned model, like LLAMA-2. As they have already been trained on text to sql datasets it is very simple to pass a prompt\\nto identify the group by column and the order by column.\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Using traditional heuristic based techniques we could identify aggreggation keywords like \"each\", \"per\", \"total\" and oredring keywords like \"sorted\", \"descending\", etc. \n",
    "Then we can check if any of the original col names occur in the questions. We can try entity linking with aggregation and ordering keywords to to try and ascertain the solution. \n",
    "This solution is very flimsy as it needs a lot of manual annotation and word maintainability.\n",
    "2. Ideally this problem can be solved by  text to text transfer where we take a model like T5 and finetune it in a pattern like \n",
    "\"Question + original sql columns[\"A\",\"B\",\"C\"] >> Group_by B order by C\" . As T5 is a language model it cann generalize to other questions also.\n",
    "3. Another method would be to gather a lot of text to sql datasets and train a T5 based model from scratch. The finetuning can be done with step 2 which will help it generalize better.\n",
    "The dataset can be augmented by doing question generation from a second model which should be verified by human annotators.\n",
    "3. The best possible current way is to use any sort of instruction tuned model, like LLAMA-2. As they have already been trained on text to sql datasets it is very simple to pass a prompt\n",
    "to identify the group by column and the order by column.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396b96b4-713c-4097-b448-a4ec75c1fb73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a2a31d-ab94-4651-a848-75fd2eb3ee22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
